---
title: "Final Code"
author: "Dylan Koproski"
date: "2025-02-21"
output: pdf_document
---
# About
This document includes ALL relevant code to the product and for Dylan's practicum. This is not restricted to NYC destinations, all destinations are included.

# Package loading
```{r}
library(tidyverse)
library(jsonlite)
library(readxl)
library(rsample)
library(caret)
library(VGAM)
library(COMPoissonReg)
library(pscl)
library(lme4)
library(zipcodeR)
library(maps)
library(MASS)
library(usmap)
library(scales)
library(geosphere)
library(glmnet)
library(rsample)
library(Metrics)
library(ModelMetrics)
library(emmeans) 

conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("map", "purrr")
conflicted::conflict_prefer("filter", "dplyr")
```

# Data loading, processing and cleaning

# Preprocessing

```{r}
# Load data
df_visitor = read_csv("mobility.csv")
df_census = read_csv("tract_census.csv", skip = 1) |> 
  janitor::clean_names()
df_tract_zip = read_excel("tract_zip.xlsx")



# Temporary restriction to NYC
state_county_code_str = c(36005, 36047, 36061, 36081, 36085)

# Process visitor data - none missing
filtered_df_visitor = df_visitor |> 
  mutate(first_five_digits = substr(poi_cbg, 1, 5)) |>
 # filter(first_five_digits %in% state_county_code_str) |> 
  # Dropping missing values for home cbg
  filter(!is.na(visitor_home_aggregation)) |> 
  mutate(identifier = row_number()) |> 
  mutate(poi_zip = postal_code) |> 
  mutate(visitor_home_aggregation = map(visitor_home_aggregation, ~fromJSON(as.character(.)))) |> 
  select(location_name, date_range_start, date_range_end, visitor_home_aggregation, top_category, identifier, poi_cbg, poi_zip, latitude, longitude) |>
  unnest_longer(visitor_home_aggregation) |>
  rename(visitor_census_tract = visitor_home_aggregation_id, visitors = visitor_home_aggregation) |> 
  mutate(visitors = if_else(visitors == 4, 3, visitors)) |> 
  mutate(poi_lat = latitude,
         poi_long = longitude)

# Census data processing
df_census = df_census |> 
  rowwise() |> 
  mutate(cbg = str_sub(geography, -11)) |> 
#873 locations have an estimated 0 people, we should exclude these.
  filter(estimate_total_total_population > 0)

# Age group proportions in census data, separate into 3 age groups
filtered_df_census_totals = 
  df_census |> 
  rowwise() |> 
  select(estimate_total_total_population, cbg, geographic_area_name, starts_with("estimate")) |> 
  mutate(
    total_under_18 = sum(estimate_total_total_population_age_under_5_years,
                         estimate_total_total_population_age_5_to_9_years,
                         estimate_total_total_population_age_10_to_14_years,
                         estimate_total_total_population_age_15_to_19_years) / estimate_total_total_population,
    
    total_19_65 = sum(estimate_total_total_population_age_20_to_24_years,
                      estimate_total_total_population_age_25_to_29_years,
                      estimate_total_total_population_age_30_to_34_years,
                      estimate_total_total_population_age_35_to_39_years,
                      estimate_total_total_population_age_40_to_44_years,
                      estimate_total_total_population_age_45_to_49_years,
                      estimate_total_total_population_age_50_to_54_years,
                      estimate_total_total_population_age_55_to_59_years,
                      estimate_total_total_population_age_60_to_64_years,
                      estimate_total_total_population_age_65_to_69_years) / estimate_total_total_population,

    total_65_plus = sum(estimate_total_total_population_age_70_to_74_years,
                        estimate_total_total_population_age_75_to_79_years,
                        estimate_total_total_population_age_80_to_84_years,
                        estimate_total_total_population_age_85_years_and_over) / estimate_total_total_population
  ) |> 
  rename("total" = estimate_total_total_population) |> 
  select(cbg, geographic_area_name, total, total_under_18, total_19_65, total_65_plus)

# Define primary ZIP code per census tract
primary_tract_zip = df_tract_zip |> 
  group_by(tract) |> 
  summarize(zip = min(zip))  # Selects the minimum ZIP as primary for simplicity

# Merge filtered_df_census_totals with filtered_df_visitor
merged_df = filtered_df_visitor |> 
  inner_join(filtered_df_census_totals, by = c("visitor_census_tract" = "cbg")) |> 
  mutate(
    visitors_under_18 = visitors * total_under_18,
    visitors_19_65 = visitors * total_19_65,
    visitors_65_plus = visitors * total_65_plus
  ) 

# Map primary ZIP codes by merging with primary_tract_zip on the census tract
final_df = merged_df |> 
  left_join(primary_tract_zip, by = c("visitor_census_tract" = "tract")) |> 
  select(location_name, date_range_start, date_range_end, top_category, 
         identifier, poi_cbg, poi_zip, visitors, visitors_under_18, 
         visitors_19_65, visitors_65_plus, zip, poi_long, poi_lat) |> 
  mutate(visitor_zip = zip)

vis_zip_lat_long = geocode_zip(final_df$visitor_zip)

final_df = final_df |> 
  left_join(vis_zip_lat_long, by = join_by(visitor_zip == zipcode)) |> 
  mutate(vis_lat = lat,
         vis_long = lng) |> 
  select(-lat, -lng)

# Rounding, can be adjusted at will
final_df = final_df |> 
  mutate(visitors_under_18 = ceiling(visitors_under_18),
         visitors_19_65 = ceiling(visitors_19_65),
         visitors_65_plus = ceiling(visitors_65_plus)) |> 
  mutate(total_visitors = visitors_under_18 + visitors_19_65 + visitors_65_plus)

# There are 2 rows in the above data that have missing values, the visitor zip is missing. We should figure out how to handle this specifically 

# Long dataframe for modelling purposes
df_long = final_df |> 
  pivot_longer(
    cols = starts_with("visitors_"),
    names_to = "age_group",
    names_prefix = "visitors_",
    values_to = "visitor_count"
  ) |> 
  mutate(top_category = factor(top_category),
         age_group = factor(age_group, levels = c("under_18", "19_65", "65_plus")))

# Data quality looks good, 2 missing zip codes may need to be handled, but data is large enough maybe dropping them won't hurt (n = 21895 vs. n = 21893)

```

```{r}
all_cat = c(
  "Accounting, Tax Preparation, Bookkeeping, and Payroll Services",
  "Activities Related to Credit Intermediation",
  "Activities Related to Real Estate",
  "Advertising, Public Relations, and Related Services",
  "Agencies, Brokerages, and Other Insurance Related Activities",
  "Architectural, Engineering, and Related Services",
  "Automotive Parts, Accessories, and Tire Stores",
  "Automotive Repair and Maintenance",
  "Bakeries and Tortilla Manufacturing",
  "Beer, Wine, and Liquor Stores",
  "Book Stores and News Dealers",
  "Building Equipment Contractors",
  "Building Finishing Contractors",
  "Building Material and Supplies Dealers",
  "Child Day Care Services",
  "Clothing Stores",
  "Consumer Goods Rental",
  "Couriers and Express Delivery Services",
  "Depository Credit Intermediation",
  "Drinking Places (Alcoholic Beverages)",
  "Drycleaning and Laundry Services",
  "Electronic and Precision Equipment Repair and Maintenance",
  "Electronics and Appliance Stores",
  "Elementary and Secondary Schools",
  "Florists",
  "Furniture Stores",
  "Gasoline Stations",
  "General Medical and Surgical Hospitals",
  "General Merchandise Stores, including Warehouse Clubs and Supercenters",
  "Glass and Glass Product Manufacturing",
  "Grocery Stores",
  "Health and Personal Care Stores",
  "Home Furnishings Stores",
  "Investigation and Security Services",
  "Jewelry, Luggage, and Leather Goods Stores",
  "Justice, Public Order, and Safety Activities",
  "Legal Services",
  "Machinery, Equipment, and Supplies Merchant Wholesalers",
  "Museums, Historical Sites, and Similar Institutions",
  "Offices of Dentists",
  "Offices of Other Health Practitioners",
  "Offices of Physicians",
  "Offices of Real Estate Agents and Brokers",
  "Other Amusement and Recreation Industries",
  "Other Financial Investment Activities",
  "Other Miscellaneous Manufacturing",
  "Other Miscellaneous Store Retailers",
  "Other Personal Services",
  "Other Professional, Scientific, and Technical Services",
  "Other Schools and Instruction",
  "Other Specialty Trade Contractors",
  "Personal and Household Goods Repair and Maintenance",
  "Personal Care Services",
  "Printing and Related Support Activities",
  "Promoters of Performing Arts, Sports, and Similar Events",
  "Radio and Television Broadcasting",
  "Religious Organizations",
  "Restaurants and Other Eating Places",
  "Shoe Stores",
  "Sound Recording Industries",
  "Special Food Services",
  "Specialized Design Services",
  "Specialty (except Psychiatric and Substance Abuse) Hospitals",
  "Specialty Food Stores",
  "Sporting Goods, Hobby, and Musical Instrument Stores",
  "Support Activities for Road Transportation",
  "Technical and Trade Schools",
  "Transit and Ground Passenger Transportation",
  "Traveler Accommodation",
  "Warehousing and Storage",
  "Wired and Wireless Telecommunications Carriers"
)

medical_services = c(
  "General Medical and Surgical Hospitals",
  "Health and Personal Care Stores",
  "Offices of Dentists",
  "Offices of Other Health Practitioners",
  "Specialty (except Psychiatric and Substance Abuse) Hospitals",
  "Offices of Physicians"
)

essential_services = c(
  "Health and Personal Care Stores",
  "Pharmacies and Drug Stores",
  "Grocery Stores",
  "Gasoline Stations",
  "Depository Credit Intermediation",
  "Public Transport Hubs",
  "Government Offices"
)

retail_shopping = c(
  "General Merchandise Stores, including Warehouse Clubs and Supercenters",
  "Clothing Stores",
  "Shoe Stores",
  "Jewelry, Luggage, and Leather Goods Stores",
  "Electronics and Appliance Stores",
  "Furniture Stores",
  "Home Furnishings Stores",
  "Specialty Food Stores",
  "Sporting Goods, Hobby, and Musical Instrument Stores",
  "Book Stores and News Dealers"
)

entertainment_recreation = c(
  "Other Amusement and Recreation Industries",
  "Museums, Historical Sites, and Similar Institutions",
  "Promoters of Performing Arts, Sports, and Similar Events",
  "Radio and Television Broadcasting",
  "Sound Recording Industries"
)

personal_services = c(
  "Personal Care Services",
  "Drycleaning and Laundry Services",
  "Other Personal Services",
  "Personal and Household Goods Repair and Maintenance"
)

hospitality_lodging = c(
  "Traveler Accommodation",
  "Bed and Breakfast Inns",
  "Resorts",
  "Extended Stay Hotels"
)

office_professional = c(
  "Accounting, Tax Preparation, Bookkeeping, and Payroll Services",
  "Legal Services",
  "Architectural, Engineering, and Related Services",
  "Agencies, Brokerages, and Other Insurance Related Activities",
  "Offices of Physicians",
  "Offices of Dentists",
  "Offices of Other Health Practitioners",
  "Real Estate Agencies"
)


target_categories = c("Drinking Places (Alcoholic Beverages)", "Restaurants and Other Eating Places", "Beer, Wine, and Liquor Stores")

# Honestly can't remember what this does, but everything breaks without it. Leave it here please
df_long_model_filtered_1 = df_long |> 
  mutate(non_restaurant = if_else(top_category %in% target_categories, "No", "Yes"))
```

# Exploratory Data Analysis (EDA)

## Zip Code Flow Matrix
```{r}
# Remove rows with NA in visitor_zip or poi_zip
zip_matrix = final_df |> 
  filter(!is.na(visitor_zip) & !is.na(poi_zip)) |> 
  group_by(visitor_zip, poi_zip) |> 
  summarize(total_visitors = sum(visitors, na.rm = TRUE)) |> 
  pivot_wider(names_from = poi_zip, values_from = total_visitors, values_fill = 0)
```
```{r, eval = FALSE}
# Convert to matrix and plot
zip_matrix_plot = zip_matrix |> 
  column_to_rownames("visitor_zip") |> 
  as.matrix() |> 
  heatmap(
    col = colorRampPalette(c("white", "red"))(100),
    scale = "none",
    main = "Zip-to-Zip Visitor Flow",
    xlab = "Destination ZIP (To)",
    ylab = "Origin ZIP (From)"
  )
```

## State to Destination ZIP in NYC
```{r}
final_df_with_state = final_df |> 
  left_join(df_tract_zip, by = c("visitor_zip" = "zip")) |> 
  select(!zip) |> 
  rename(visitor_state = usps_zip_pref_state)

state_zip_matrix = final_df_with_state |> 
  group_by(visitor_state, poi_zip) |> 
  summarize(total_visitors = sum(visitors, na.rm = TRUE)) 

ggplot(state_zip_matrix, aes(x = poi_zip, y = visitor_state, fill = total_visitors)) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  labs(
    title = "State-to-NYC ZIP Code Visitor Flow",
    x = "Destination ZIP (NYC)",
    y = "Origin State"
  ) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```


```{r, eval = FALSE}
nyc_zip_visitors = final_df |> 
  group_by(poi_long, poi_lat) |> 
  summarize(total_visitors = sum(visitors, na.rm = TRUE)) 


ggplot(nyc_zip_visitors, aes(x = poi_long, y = poi_lat)) +
  stat_density_2d(aes(fill = after_stat(level)), geom = "polygon", color = NA) +
  scale_fill_viridis_c() +
  labs(
    title = "Heatmap of NYC POIs by Visitor Counts",
    x = "Longitude", 
    y = "Latitude"
  ) +
  theme_minimal()
```

## Flow map - restrict to state of interest
Illustrative, shows that not every states have visitors from all states, including NYC. Illustrates the need to include POI types from all states.

```{r}
#excise visitors from hawaii, include the second filter argument to get a better look at lower ends of data
flow_df = final_df |> 
  filter(vis_long > -150) |> 
  filter(poi_long > -150) |> 
  filter(vis_lat < 60) |> 
  filter(poi_lat < 50) |> 
  left_join(df_tract_zip, by = c("visitor_zip" = "zip"), multiple = "any") |> 
  select(!zip) |> 
  rename(visitor_state = usps_zip_pref_state) |> 
  left_join(df_tract_zip, by = c("poi_zip" = "zip"), multiple = "any") |> 
  rename(poi_state = usps_zip_pref_state) |> filter(poi_state == "TN")

# if you want to include visitors from hawaii
# flow_df = final_df

usa = map_data("state")

usa = rename(usa, state = "region")

usa$state = str_to_title(usa$state)

stateData = usa |> 
  arrange(state, group, order)

ggplot() +
  geom_polygon(data = stateData,
               aes(x = long, y = lat, group = group),
               fill = "white", color = "gray50") +
  
  geom_segment(data = flow_df,
               aes(x = vis_long, y = vis_lat,
                   xend = poi_long, yend = poi_lat,
                   color = total_visitors),
               alpha = 0.6, linewidth = 1) +
  scale_color_fermenter(name = "Total Visitors", direction = -1) +
  coord_map() +
  theme_minimal() +
  labs(color = "Volume of Visits",
       title = "Zip Code to Zip Code Visits",
       x = "Longitude",
       y = "Latitude")


```

## Visitor counts

```{r}
state_visitors = final_df_with_state |> 
 # filter(visitor_state != "NY") |> 
#  filter(visitor_state != "NJ") |> 
  group_by(visitor_state) |> 
  summarize(total_visitors = sum(visitors, na.rm = TRUE)) |> 
  arrange(desc(total_visitors))

ggplot(state_visitors, aes(x = reorder(visitor_state, -total_visitors), y = total_visitors)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Total Visitors by State",
    x = "State",
    y = "Total Visitors"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Map view of visitor counts

```{r}
# Aggregate visitor counts by state
state_visitors_map = final_df_with_state |> 
  group_by(visitor_state) |> 
  summarize(total_visitors = sum(visitors, na.rm = TRUE)) |> 
  filter(!is.na(visitor_state)) |> 
  mutate(log_visitors = log1p(total_visitors)) |> 
  rename(state = visitor_state)

# Plot using log scale
plot_usmap(data = state_visitors_map, regions = "states", values = "log_visitors") +
  scale_fill_viridis_c(name = "Log Visitors", option = "magma") +
  labs(title = "Log-Scaled Visitor Counts by State") +
  theme_minimal()

```

```{r}
# Aggregate visitor counts by age group and POI category
df_filtered = df_long |> 
  mutate(category_group = case_when(
    top_category %in% medical_services ~ "Healthcare",
    top_category %in% essential_services ~ "Essential Services",
    top_category %in% retail_shopping ~ "Retail Shopping",
    top_category %in% entertainment_recreation ~ "Entertainment/Recreation",
    top_category %in% personal_services ~ "Personal Services",
    top_category %in% hospitality_lodging ~ "Hospitality/Lodging",
    top_category %in% office_professional ~ "Office/Professional",
    top_category %in% target_categories ~ "Restaurant/Bar",
    TRUE ~ "Other"
  )) |> 
  filter(category_group != "Other") |>  # Exclude any unintended categories
  group_by(category_group, age_group) |> 
  summarize(total_visitors = sum(visitor_count, na.rm = TRUE), .groups = "drop")

# Create stacked bar plot
ggplot(df_filtered, aes(x = category_group, y = total_visitors, fill = age_group)) +
  geom_col(position = "stack") +
  labs(
    title = "Visitor Counts by POI Category and Age Group",
    x = "POI Category",
    y = "Total Visitors",
    fill = "Age Group"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.position = "right"  # Keep legend for age group colors
  )

```

## Pie chart
Simple visual to include in report, just shows the breakdown of age representation. Illustrates the need for an offset parameter in models.

```{r}
age_group_proportions = df_long_model_filtered_1 |>
  group_by(age_group) |>
  summarize(total_visitors = sum(visitor_count), .groups = "drop") |>
  mutate(proportion = total_visitors / sum(total_visitors))

ggplot(age_group_proportions, aes(x = "", y = proportion, fill = age_group)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  labs(
    title = "Proportion of Visitors by Age Group",
    fill = "Age Group"
  ) +
  theme_void()

```


# Hypothesis testing
## Part 1: POI restricted age group visitation analysis
Wanted to use poisson, ideal for count modelling. However, with overdispersion we use NB as it allows for multiple parameters to handle different variance from mean.

These models are not predictive, they answer the question: Does age group affect visits differently across POI types? Or in other words, when restricting data to a certain poi type, are we seeing different visitation rates.
```{r}
run_nb_model = function(df, category_name, category_vector) {
  df_model = df |> 
    filter(top_category %in% category_vector) |>  # Filter to only relevant POIs
    mutate(
           age_group = factor(age_group, levels = c("under_18", "19_65", "65_plus")))  # Ensure age group ordering
  
  nb_model = glm.nb(visitor_count ~ age_group + offset(log(total_visitors)), data = df_model)

  print(summary(nb_model))
  
  return(nb_model)
}

# Run models for each POI type vs. Restaurant/Bar
nb_model_1 = run_nb_model(df_long, "Full", all_cat)
nb_model_2 = run_nb_model(df_long, "Healthcare", medical_services)
nb_model_3 = run_nb_model(df_long, "Essential Services", essential_services)
nb_model_4 = run_nb_model(df_long, "Retail Shopping", retail_shopping)
nb_model_5 = run_nb_model(df_long, "Entertainment/Recreation", entertainment_recreation)
nb_model_6 = run_nb_model(df_long, "Personal Services", personal_services)
nb_model_7 = run_nb_model(df_long, "Hospitality/Lodging", hospitality_lodging)
nb_model_8 = run_nb_model(df_long, "Office/Professional", office_professional)
nb_model_9 = run_nb_model(df_long, "Restaurant/Bar", target_categories)
```

```{r}
extract_nb_results = function(model, category_name) {
  results = broom.mixed::tidy(model) |> 
    mutate(significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01  ~ "**",
      p.value < 0.05  ~ "*",
      TRUE ~ ""
    ))
  
  return(results)
}

# Combine results from all models
nb_summary_table = bind_rows(
  extract_nb_results(nb_model_1, "Non-Restaurant"),
  extract_nb_results(nb_model_2, "Healthcare"),
  extract_nb_results(nb_model_3, "Essential Services"),
  extract_nb_results(nb_model_4, "Retail Shopping"),
  extract_nb_results(nb_model_5, "Entertainment/Recreation"),
  extract_nb_results(nb_model_6, "Personal Services"),
  extract_nb_results(nb_model_7, "Hospitality/Lodging"),
  extract_nb_results(nb_model_8, "Office/Professional"),
  extract_nb_results(nb_model_9, "Restaurant/Bar")
)

nb_summary_table = nb_summary_table |> 
  bind_cols(category = c("Full","Full","Full", "Healthcare", "Healthcare", "Healthcare", "Essential Services","Essential Services","Essential Services", "Retail Shopping","Retail Shopping","Retail Shopping", "Entertainment/Recreation","Entertainment/Recreation","Entertainment/Recreation", "Personal Services","Personal Services","Personal Services", "Hospitality/Lodging","Hospitality/Lodging","Hospitality/Lodging", "Office/Professional","Office/Professional","Office/Professional", "Restaurant/Bar","Restaurant/Bar","Restaurant/Bar")) |> 
  relocate(category)

# Display as a table
knitr::kable(nb_summary_table)

```

## Part 2: Age group restricted POI analysis

It is now known that visitations differ by age group. When restricting to different categories, we see a noticeable trend that middle group visits most, <18 next and 65+ least. So now I ask, when restricting to one age group, are we seeing differences in visitation rates when comparing poi types? E.g. does age group 1 visit poi type x at a different rate than poi type y?

```{r}
  

# Making a poi type column (this would've made the other analysis above a bit easier, however, it is already done and I am afraid of breaking this further. So we will persis like this).
df_long = df_long |> mutate(poi_category = case_when(
  top_category %in% medical_services ~ "Healthcare",
  top_category %in% essential_services ~ "Essential Services",
  top_category %in% retail_shopping ~ "Retail Shopping",
  top_category %in% entertainment_recreation ~ "Entertainment/Recreation",
  top_category %in% personal_services ~ "Personal Services",
  top_category %in% hospitality_lodging ~ "Hospitality/Lodging",
  top_category %in% office_professional ~ "Office/Professional",
  top_category %in% target_categories ~ "Restaurant/Bar",
  TRUE ~ "Other"
))

run_nb_model = function(df, age_group_label) {
  df_filtered = df |> filter(age_group == age_group_label)
  
  nb_model = glm.nb(visitor_count ~ poi_category + offset(log(total_visitors)), 
                    data = df_filtered)
  
  return(nb_model)
}

nb_model_under_18 = run_nb_model(df_long, "under_18")
nb_model_19_65 = run_nb_model(df_long, "19_65")
nb_model_65_plus = run_nb_model(df_long, "65_plus")

extract_emmeans = function(model, age_group_label) {
  em_results = emmeans(model, pairwise ~ poi_category, adjust = "bonferroni")
  return(as.data.frame(em_results$contrasts) |> mutate(age_group = age_group_label))
}

em_under_18 = extract_emmeans(nb_model_under_18, "under_18")
em_19_65 = extract_emmeans(nb_model_19_65, "19_65")
em_65_plus = extract_emmeans(nb_model_65_plus, "65_plus")

em_results_df = bind_rows(em_under_18, em_19_65, em_65_plus)

print(em_results_df)

```

# Predictive models

Now with inference out of the way, the final step here is to produce predictive models to determine how many visitors in a given age group we should expect given a certain POI type, total number of visitors and distance traveled. The result is three gravity models built using ridge regression. We perform a data split here. It is not super important to do the split since the model is cross validated using ridge, however, I want to see how the model fits new data. The data split provides an honest description of how the model will fit a new dataset. NOTE: ridge is selected over lasso because lasso shrinks low-priority coefficients to 0. Because of the large volume of POI types (many of which are insignificant). Ridge brings the most insignificant ones close to 0, but does not remove them completely. Ridge removes an error where lasso models would just have no parameters, despite the fact that many are important.

```{r}
df_long$distance_km = distHaversine(
  matrix(c(df_long$vis_long, df_long$vis_lat), ncol = 2),
  matrix(c(df_long$poi_long, df_long$poi_lat), ncol = 2)
) / 1000  

set.seed(2025)  # Reproducibility

df_long = df_long |> drop_na()  # Drop missing values, this was breaking ridge

# Split dataset into training and testing
train_index = createDataPartition(df_long$visitor_count, p = 0.8, list = FALSE)
train_data = df_long[train_index, ]
test_data = df_long[-train_index, ]

```


```{r, eval = FALSE}
# Filter for age group
train_under_18 = train_data |> filter(age_group == "under_18")
test_under_18 = test_data |> filter(age_group == "under_18")

X_train_under_18 = model.matrix(visitor_count ~ top_category + total_visitors + distance_km, data = train_under_18)[, -1]
X_test_under_18 = model.matrix(visitor_count ~ top_category + total_visitors + distance_km, data = test_under_18)[, -1]
y_train_under_18 = train_under_18$visitor_count
y_test_under_18 = test_under_18$visitor_count

ridge_cv_under_18 = cv.glmnet(X_train_under_18, y_train_under_18, alpha = 0, family = "poisson")
best_lambda_ridge_under_18 = ridge_cv_under_18$lambda.min
ridge_under_18 = glmnet(X_train_under_18, y_train_under_18, alpha = 0, family = "poisson", lambda = best_lambda_ridge_under_18)

pred_ridge_under_18 = predict(ridge_under_18, X_test_under_18, type = "response")

rmse_ridge_under_18 = sqrt(mean((y_test_under_18 - pred_ridge_under_18)^2))
cat("Ridge RMSE for Under 18:", rmse_ridge_under_18, "\n")


### Ridge for 19-65
train_19_65 = train_data |> filter(age_group == "19_65")
test_19_65 = test_data |> filter(age_group == "19_65")

X_train_19_65 = model.matrix(visitor_count ~ top_category + total_visitors + distance_km, data = train_19_65)[, -1]
X_test_19_65 = model.matrix(visitor_count ~ top_category + total_visitors + distance_km, data = test_19_65)[, -1]
y_train_19_65 = train_19_65$visitor_count
y_test_19_65 = test_19_65$visitor_count

ridge_cv_19_65 = cv.glmnet(X_train_19_65, y_train_19_65, alpha = 0, family = "poisson")
best_lambda_ridge_19_65 = ridge_cv_19_65$lambda.min
ridge_19_65 = glmnet(X_train_19_65, y_train_19_65, alpha = 0, family = "poisson", lambda = best_lambda_ridge_19_65)

pred_ridge_19_65 = predict(ridge_19_65, X_test_19_65, type = "response")
rmse_ridge_19_65 = sqrt(mean((y_test_19_65 - pred_ridge_19_65)^2))
cat("Ridge RMSE for 19-65:", rmse_ridge_19_65, "\n")

### Ridge for 65+
train_65_plus = train_data |> filter(age_group == "65_plus")
test_65_plus = test_data |> filter(age_group == "65_plus")

X_train_65_plus = model.matrix(visitor_count ~ top_category + total_visitors + distance_km, data = train_65_plus)[, -1]
X_test_65_plus = model.matrix(visitor_count ~ top_category + total_visitors + distance_km, data = test_65_plus)[, -1]
y_train_65_plus = train_65_plus$visitor_count
y_test_65_plus = test_65_plus$visitor_count

ridge_cv_65_plus = cv.glmnet(X_train_65_plus, y_train_65_plus, alpha = 0, family = "poisson")
best_lambda_ridge_65_plus = ridge_cv_65_plus$lambda.min
ridge_65_plus = glmnet(X_train_65_plus, y_train_65_plus, alpha = 0, family = "poisson", lambda = best_lambda_ridge_65_plus)

pred_ridge_65_plus = predict(ridge_65_plus, X_test_65_plus, type = "response")
rmse_ridge_65_plus = sqrt(mean((y_test_65_plus - pred_ridge_65_plus)^2))
cat("Ridge RMSE for 65+:", rmse_ridge_65_plus, "\n")


# Extract and display non-zero coefficients for Under 18
cat("\nSelected Variables for Under 18:\n")
print(ridge_coefs_under_18_df |> filter(coefficient != 0))

cat("\nSelected Variables for 19-65:\n")
print(ridge_coefs_19_65_df |> filter(coefficient != 0))

cat("\nSelected Variables for 65+:\n")
print(ridge_coefs_65_plus_df |> filter(coefficient != 0))

```
